{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP3-Correction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HpBfNfko8tzO",
        "yugwU2csH-eU",
        "FDH-6i_bRjsS",
        "pHGg1qzM-LhP",
        "OYqh-rfE8gmJ",
        "LYd8QAuiNeu_",
        "G_Ls5p6qZ0jz"
      ],
      "authorship_tag": "ABX9TyOez3LHJffH7+j4KSKvX2KK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/securitylab-repository/TPS-IA/blob/master/TP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKFzudNXTPRV"
      },
      "source": [
        "# TP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMi0DfteSpOE"
      },
      "source": [
        "***L'objectif de ce TP est de vous montrer comment mener un projet de machine learning depuis la récupération du dataset jusqu'à l'étape de test en passant par la phase d'apprentissage.***\n",
        "\n",
        "***Référez vous à la correction du [TP1](https://github.com/securitylab-repository/TPS-IA/blob/master/TP1-Correction.ipynb) en ce qui concerne les librairies  numpy, pandas, matplotlib et ScyPi.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob09ED6OTKec"
      },
      "source": [
        "## Récupération du dataset\n",
        "C'est une étape que nous avons eu l'occasion de voir lors des TPs précedents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwh_IC2VTJqP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "housing = pd.read_csv('https://raw.githubusercontent.com/securitylab-repository/TPS-IA/master/datasets/housing.csv',delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R65WrbwfUBRI"
      },
      "source": [
        "## Comprendre la structure du dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGJInZWuUWMI"
      },
      "source": [
        "1. Affichage des cinq premiers exemples d'entrainement avec la fonction `head()` pour connaître les noms et le nombre d'attributs (features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYydI7-UJoS"
      },
      "source": [
        "housing.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVmwO3KZU3wZ"
      },
      "source": [
        "2. Il est également utile de connaître le nombre d'exemples d'entrainement, le nombre de valeurs non nulles et le type de chaque caractéristique (feature, colonne, attribut). A cet effet nous utilisons la méthode `info`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PY-YWsFXxE_"
      },
      "source": [
        "> Que remarquez-vous ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ineSy2Q-WfNv"
      },
      "source": [
        "housing.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8j0bRGrWlSz"
      },
      "source": [
        "***Réponse***\n",
        "\n",
        "D'après le résultat de la fonction `info`, nous remarquons deux choses importantes:\n",
        "- tous les features sont de type `float64` sauf l'attribut `ocean_proximity` qui est de type `object` (probablement un objet chaîne de caractères, car le `DataFrame` a été obtenu depuis un fichier CSV),\n",
        "\n",
        "- il existe 207 exemples d'entrainement (lignes) qui ont une valeur nulle du feature `total_bedrooms`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpkOfI4XZOS_"
      },
      "source": [
        "3. Sélectionnez un échantillon de quelques lignes et n'affichez que l'attribut `ocean_proximity'\n",
        "\n",
        "> Que remarquez-vous ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLB6KuPOdOPU"
      },
      "source": [
        "display(housing['ocean_proximity'].sample(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghz7UoyrfIO6"
      },
      "source": [
        "***Réponse***\n",
        "\n",
        "On remarque que les valeurs de cet attribut se répétent, on est donc face à des catégories. \n",
        "\n",
        "4. Affichons le nombre d'occurences de chaque catégorie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PLhCQWhf4OA"
      },
      "source": [
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWQgioHgDku"
      },
      "source": [
        "5. Affichage de la distribution (statistique) de tous les features numériques en utilisant la fonction `describe`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rau-CY4og6K1"
      },
      "source": [
        "housing.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKLmwi3KhYDY"
      },
      "source": [
        "Au delà des informations statistiques du dataset, la chose importante que nous pouvons noter ici, et qui influe considérablement sur la convergence des algorithmes d'apprentissage (notamment la décente en gradient), est que les features ont des échelles de valeurs différentes.  Ce qui nécessiterait par exemple une normalisation de tous les features (nous verrons cela plus loin)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xjt5Xcqp6i2"
      },
      "source": [
        "6. Une autre façon d'analyser la distribution des valeurs de chaque feature est de dessiner l'hystogramme de chaque feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRRDpn-RhZCv"
      },
      "source": [
        "#%matplotlib inline   # Seulement dans Jupyter notebook\n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUMlZt6N45-0"
      },
      "source": [
        "> Que remarquez-vous ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNS_ymKzhwU"
      },
      "source": [
        "***Réponse***\n",
        "\n",
        "En plus des remarques soulevées précédement, nous constatons que les valeurs du feature `housing_median_age` et `media_house_value` sont plafonnées. Ceci peut causer de sérieux problèmes lors de l'apprentissage, surtout pour la colonne `media_house_value` qui est la cible à prédire. En effet, les algorithmes d'apprentissage risquent d'apprendre que les prix des maisons ne dépassent 500 000 euro.\n",
        "\n",
        "La solutions est soit de revenir à l'état initial pour les exemples d'entrainement (lignes) concernés par ce plafonnement, soit de les supprimer de l'ensemble d'entrainement et de test.\n",
        "\n",
        "![Texte alternatif…](https://github.com/securitylab-repository/TPS-IA/raw/master/histogramme_interpretation.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpBfNfko8tzO"
      },
      "source": [
        "## Création de l'ensemble d'entrainement et de test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnCNKKgwBT3x"
      },
      "source": [
        "Lorsqu'on construit notre ensemble d'entrainement et de test, on fait attention au points suivants:\n",
        "\n",
        "- En général on réserve 20% des données au jeux de test. Moins de 20% si le jeux de données est très large. Par conséquent 80% à l'ensemble d'entrainement.\n",
        "\n",
        "- Le tirage des exemples doit être aléatoire et ne doit pas changer d'une exécution à une autre. Pour cela, le tirage pseudo aléatoire utilisé doit être paramétré avec une graine (seed) fixe.\n",
        "\n",
        "- Le tirage doit prendre en considération les mises-à-jour du dataset et rester cohérent avec les tirages précédents (prendre seulement 20% parmi les nouveaux exemples d'entrainement).\n",
        "\n",
        "Pour construire les ensembles d'entrainement et de test respectant ces contraintes, on utilisera la fonction `train_test_split` de la librairie `scikit-learn`.\n",
        "\n",
        "> Que représente les arguments `test_size` et `random_state` ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRzQsebgG3Mg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "display(type(train_set))\n",
        "display(type(test_set))\n",
        "display(type(train_set.info()))\n",
        "display(type(test_set.info()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCcHe5lG9bZ"
      },
      "source": [
        "- Si le dataset n'est pas suffisament large, un simple tirage aléatoire ne suffit plus. Il faut prendre en cosidération le pourcentage de chaque catégorie influente de notre dataset de départ. Par exemple, si notre dataset sont les réponses d'un sondage sur une population constituée de 60% de fêmmes et 40% d'hommes, il est recommandé de tenir compte de ces pourcentage de départ dans la constitution de nos ensembles d'entrainement et de test. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRI0Zh0IafGP"
      },
      "source": [
        "Supposons que selon un expert, l'attribut `media_income` est très déterminant dans la prédiction du prix d'une maison (l'attribut `median_house_value`). D'après l'histogramme de  `media_income`, nous pouvons remarquer que la majorité des exemples ont des valeurs comprises entre 1.5 et 6, mais il existe aussi des exemples dont les valeurs sont plus grande que 6. Il est donc intéressant  de piocher les exemples dans le dataset proportionnellement au nombre d'exemples dans chaque plage d'intervalle (catégorie à définir).\n",
        "![Texte alternatif…](https://github.com/securitylab-repository/TPS-IA/raw/master/median_income.png)\n",
        "\n",
        "A cet effet, nous utiliserons la fonction `pd.cut()`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pVZqGrPeU5-"
      },
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "housing[\"income_cat\"].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNV-W8_Ef6oW"
      },
      "source": [
        "Nous allons maintenant reconstruire nos ensembles d'entrainement et de test en prenant en considération ces nouvelles catégories et en utilisant à présent la fonction `StratifiedShuffleSplit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qgH4PQgXos"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "\n",
        "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdVN_6FIhNgS"
      },
      "source": [
        "Supprimer maintenant la colonne que nous venons de créer pour rétablir l'état du dataset originel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv6j-tWrhOXq"
      },
      "source": [
        "for set_ in (strat_train_set, strat_test_set): \n",
        "     set_.drop(\"income_cat\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yugwU2csH-eU"
      },
      "source": [
        "##  Analyse de la corrélation des données\n",
        "### Coéfficient de corrélation standard (Pearson's R)\n",
        "\n",
        "Dans le cas où le dataset n'est pas très large, il est intéressant de claculer la matrice de corrélation entre tous les attributs du dataset et en particulier entres les attributs et la cible.\n",
        "\n",
        "Pour claculer la matrice de corrélaiton, on utilise la fonction `corr()` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1kezrgZKqP0"
      },
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET2uhLTvMc4B"
      },
      "source": [
        "> Interpréter les résultats ci-dessus ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyBB9tlIMsK9"
      },
      "source": [
        "Une autre manière d'analyser les dépendances entre attributs est d'utiliser la fonction `scatter_matrix()`. Qui permet de visualiser graphiquement les attributs deux-à-deux. Le faire en prenant en considération tous les attributs, serait fastidieux. \n",
        "\n",
        "En général, on l'applique que sur les attributs les plus prometteurs (ceux qui auraient donnés de bons résultats avec la première méthode par exemple). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO9EhWN-Ojm_"
      },
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhSrNVyAPoQX"
      },
      "source": [
        "> Ce résulat confirme-t-il celui obtenu avec la matrice de corrélation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDhZvxoOxFv"
      },
      "source": [
        "On remarque que l'attribut qui montre le plus de corrélation avec notre cible `mediane_house_value` est `median_income`. Analysons de plus près le graphique associé.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2bIZtlcQfyj"
      },
      "source": [
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8A3O0fQroz"
      },
      "source": [
        "> Quels problèmes remarquez-vous d'après ce résultat et comment y remédier ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDH-6i_bRjsS"
      },
      "source": [
        "## Extraction des étiquettes (cibles)\n",
        "\n",
        "Avant de continuer, il est utile de séparer les étiquettes du reste du `DataFrame`, car les transformations que nous allons effectuer sur l'un ou l'autre seront différentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrjLk43fwNZS"
      },
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "labels = strat_train_set[\"median_house_value\"].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHGg1qzM-LhP"
      },
      "source": [
        "## Nettoyage du dataset\n",
        "\n",
        "Certains algorithmes de machine learning ne tolèrent pas de valeurs manquantes sur le dataset. Par exemple, dans le cas de ce dataset, l'attribut  `total_bedrooms` possèdent des valeurs manquantes.\n",
        "\n",
        "Pour remédier à cela, nous avons trois solutions:\n",
        "\n",
        "- Supprimer les lignes dont certaines valeurs sont manquantes\n",
        "- Supprimer complètement l'attribut concerné \n",
        "- Mettre une autre valeur à la place du `null` (moyenne, médiane, zéro,...). Si on choisit cette dernière version, il faudrait sauvegarder la mediane calculée sur l'ensemble d'entrainement pour l'appliquer également sur l'ensemble de test. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H-TWzwS2CkD"
      },
      "source": [
        "housing.isnull().sum()\n",
        "#housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n",
        "#housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
        "\n",
        "#median = housing[\"total_bedrooms\"].median()  # option 3\n",
        "#housing[\"total_bedrooms\"].fillna(median, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Axg9tYO2h0B"
      },
      "source": [
        "Une autre façon de procéder est d'utiliser la classe `SimpleImputer`. Par contre elle fonctionne que sur des valeurs numériques. Il faut donc créer une copie des données sans l'attribut `ocean_proximity`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZrJLZr036YR"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# faire un copie sans l'attribut ocean_proximity\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "imputer.fit(housing_num)\n",
        "\n",
        "print(imputer.statistics_)\n",
        "\n",
        "print(housing_num.median().values)\n",
        "\n",
        "X = imputer.transform(housing_num)\n",
        "\n",
        "print(type(X))\n",
        "\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n",
        "display(housing_tr.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6r3iJ1DAX_9"
      },
      "source": [
        "> Vérifier qu'il n'existe plus de valeurs manquantes dans `housing_tr`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCMpPZxJDqne"
      },
      "source": [
        "## Attribut sous forme de catégorie\n",
        "\n",
        "Comme relevé ci-dessus, l'attribut `ocean_proximity` est de type `chaîne de caractères`, mais ses valeurs se répétent et ne sont pas nombreuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt3_vf8oEdoK"
      },
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7H3zYDEd45"
      },
      "source": [
        " On peut donc les regrouper par catégorie et représenter chaque catégorie par un entier. A cet effet, ont peut utiliser la classe `OrdinalEncoder` de `Scikit-Learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzo02MVBFDjz"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "print(housing_cat_encoded[:10])\n",
        "print(ordinal_encoder.categories_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4SM756TJ1wg"
      },
      "source": [
        "Une autre manière de faire est de créer autant de colonnes que de catégories. Pour chaque ligne, la valeur de chaque colonne est 1 si la ligne appartient à cette catégorie ou 0 dans le cas contraire. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPVD_qVyJ2PN"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "print(housing_cat_1hot)\n",
        "#print(housing_cat_1hot.toarray())\n",
        "#print(cat_encoder.categories_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvYbuXREOuDH"
      },
      "source": [
        "> Comparez les deux méthodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnzdgJR3LolS"
      },
      "source": [
        "Une tout autre solution est de complètement modifier les valeurs de cette colonne en mettant à sa place des valeurs numériques proches de la signification de cette colonne. Dans cet exemple, on peut prendre par exemple la distance par rapport à l'océan. Ceci donnerait un sens proche de celui que représentent les catégories. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYqh-rfE8gmJ"
      },
      "source": [
        "## Transformation des features\n",
        "\n",
        "Il arrive assez souvent qu'on veuille transformer ou même ajouter de nouveaux features plus pertinents à notre dataset.\n",
        "\n",
        "Pour ce faire et afin d'automatiser cette tâche, Nous allons créer une classe permettant de le faire. \n",
        "\n",
        "Cette classe va hériter des classes  `BaseEstimator`, `TransformerMixin` afin de mieux l'intégrer à scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEzgzEmy_D0h"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household,\n",
        "                         bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH-Hvos9_Vno"
      },
      "source": [
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
        "    index=housing.index)\n",
        "housing_extra_attribs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Mhe9NYMqqQ"
      },
      "source": [
        "> Expliquez ce que permet de réaliser cette classe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYd8QAuiNeu_"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Comme indiqué dans le cours, il est important pour certains algorithmes d'apprentissage automatique d'opérer sur des données dont les valeurs sont à-peu près dans la même echelle de grandeur. Il existe deux méthodes pour remédier à ce problème :\n",
        "- La standardisation (On peut utiliser la classe `StandardScaler`)\n",
        "- La normalisation:  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMkgDsfVVfL2"
      },
      "source": [
        "housing_mean = housing.drop(\"ocean_proximity\", axis=1)\n",
        "for col in housing.columns:\n",
        "  if (col != \"ocean_proximity\"):\n",
        "    housing_mean[col] = (housing_mean[col] - housing_mean[col].median()) / (housing_mean[col].max() - housing_mean[col].min())\n",
        "\n",
        "print(housing_mean.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D_PEX-fYK6g"
      },
      "source": [
        "> Donnez la différence entre les deux méthodes en demandant à google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Ls5p6qZ0jz"
      },
      "source": [
        "## Transformation en Pipelines\n",
        "\n",
        "Vous avez remarqué qu'il y a de nombreuses transformations que nous devons réaliser dans un ordre bien précis. Heureusement que Scikit-learn offre un moyen automatisé pour le faire et dans le bon ordre. Il le fait à travers la classe `Pipeline`.\n",
        "\n",
        "Exemple: Effectuons les transformations suivantes sur les features numériques en nous aidant d'un `Pipeline`:\n",
        "- Supprimer les lignes dont certaines valeurs sont `NaN`\n",
        "- Combiner certains attributs \n",
        "- Standardisation\n",
        "\n",
        "Le constructeur de la classe `Pipeline` admet une liste d'objets appelés `estimateur/transformer`. La particularité de ces objets est qu'ils doivent posséder une fonction `fit_transform()` et `fit()`. \n",
        "\n",
        "La classe `PipeLine` appelle  dansl l'ordre la fonction `fit_transform()` de chaque estimateur de la liste en lui transmettant à chaque fois le résultat retourné par l'estimateur précédent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgUuLY_bM3II"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
        "print(housing_num_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1OaXjNUXw1V"
      },
      "source": [
        "> D'où vient la classe `CombinedAttributesAdder` ?\n",
        "\n",
        "Et si on veut traiter au même temps les features numériques et non numériques, on peut utiliser la classe `ColumnTransformer`. Contrairement à la classe `Pipeline`, son constructeur admet en plus la liste des features concernés par la transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIoAh8a9Xvgb"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)\n",
        "print(housing_prepared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1PqU0_lt4g"
      },
      "source": [
        "## Entraînement et évaluation sur l'ensemble d'entraînement\n",
        "\n",
        "Nous allons utiliser ici la régression linéaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhfw2CVQmEFV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad59d3af-5e0c-427d-dcd9-59aa7d208ab2"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLA9ERp6nq1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1c422dc3-4d2d-494c-f8e8-5fd1ab591d11"
      },
      "source": [
        "some_data = housing.iloc[:5]\n",
        "some_labels = labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions: [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n",
            " 189747.55849879]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ff-ZtmdoRdT"
      },
      "source": [
        "Essayons maintenant sur l'ensemble du dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsscWPK2oQ9Z"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK-ua5H0od7N"
      },
      "source": [
        "> Que pensez vous du résultat obtenu, sommes-nous face à un overfiting ou underfiting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_XzKo6Sondx"
      },
      "source": [
        "Essayon un autre algorithme qui donne de meilleurs résultats que ce soit sur des données linéairement séparables ou non, à savoir les arbres de décision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiI3MjDG6XMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "91ac5156-5faa-4b01-f9f3-56ce19a04345"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(housing_prepared, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsfeQSUb69RK"
      },
      "source": [
        "Evaluons maintenant l'algorithme sur les données d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AyL6w7J7CCX"
      },
      "source": [
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXIIjRBs7UFC"
      },
      "source": [
        "Que pensez vous du résultat obtenu, sommes-nous face à un overfiting ou à un underfiting ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqVvLhORc2xt"
      },
      "source": [
        "## Cross validation\n",
        "\n",
        "Il arrive souvent que le choix de l'algorithme (Estimateur) à utiliser n'est pas évident. Pour choisir la meilleure solution à entraîner avant la phase de test, on utilise ce qu'on appelle la validation croisée (ex. `K-fold cross-validation`). On subdivise le dataset d'entrainement en $n$ partitions (appelées `folds`) et on réalise $k$ itérations entrainement/validation. A chaque itération, on choisit un `fold` différent pour les tests et les `k-1` restant pour l'entrainement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvy6MFzhH_Z"
      },
      "source": [
        "### Validation croisée (Régression linéaire)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dE3aAHeiGr1"
      },
      "source": [
        "def display_scores(scores):\n",
        "  print(\"Scores:\", scores)\n",
        "  print(\"Mean:\", scores.mean())\n",
        "  print(\"Standard deviation:\", scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g6kpirqhWcJ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, labels,\n",
        "scoring=\"neg_mean_squared_error\", cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0t2mOZ_iNKs"
      },
      "source": [
        "### Validation croisée (Arbre de décision)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US3ybivpf2Cg"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, housing_prepared, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Uq_QxKhE5h"
      },
      "source": [
        "> Que signifie la ligne `Standrd deviation` ? \n",
        "\n",
        "> Pourquoi avons-nous de meilleurs résultats avec la régression linéaire ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC1azNWsiqok"
      },
      "source": [
        "### Validation croisée (Random Forest) \n",
        "\n",
        "L'algorithme `Random Forest` est réputé pour donner de meilleurs résultats que les simples arbres de décision. Il construit plusieurs arbres de décision sur un sous ensembles de features à chaque fois et réalise une moyenne.\n",
        "\n",
        "> Réalisez l'apprentissage d'un algorithme `Random Forest` (utilisez la classe `RandomForestRegressor`) \n",
        "\n",
        "> Faites une prédiction sur l'ensemble d'entrainement. Sommes-nous face à un overfiting ou à un underfiting ?\n",
        "\n",
        "> Faites une validation croisée \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6s_ZxH0nXiX"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(housing_prepared, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5FDWle6nz6u"
      },
      "source": [
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzbWTiCZoQxU"
      },
      "source": [
        "scores = cross_val_score(forest_reg, housing_prepared, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "display_scores(forest_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm9ilkvEqDar"
      },
      "source": [
        "##  Grid Search\n",
        "\n",
        "Une fois un ou plusieurs algorithmes sélectionnés, nous aurons souvent, selon les algorithmes, certains hyper-paramètres à positionner. Par exemple, nous utiliserons ici la validation croisée pour choisir les meilleures valeurs des hyper-paramètres `n_estimators` et `max_features` de l'algorithme `Random Forest`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQ7GqJy49gF"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Dictionnaire définissant les paramètres à évaluer\n",
        "param_grid = [\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor()\n",
        "\n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(housing_prepared, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGZmZFFZWq8O"
      },
      "source": [
        "Affichage des meilleurs valeurs des hyper-paramètres trouvés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QGiWOjG5FBH"
      },
      "source": [
        "grid_search.best_params_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1aHSDziW8RU"
      },
      "source": [
        "Sauvegrde de la meilleure configuration de l'algorithme "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqqQH9F5L_3"
      },
      "source": [
        "model_final = grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkgXoC8k6Auu"
      },
      "source": [
        "## Sélection des meilleurs features\n",
        "\n",
        "Une autre façon d'utiliser la validation croisée est de sélectionner les features les plus influents dans l'apprentissage afin de réduire la dimensionnalité du dataset. Ceci est souvent nécessaire quand le nombre de features est très important. Ce n'est pas vraiment le cas ici. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZec_iFT8DHT"
      },
      "source": [
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFKgv9KD8bI1"
      },
      "source": [
        "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YCc_HEHqPi9"
      },
      "source": [
        "## Evaluation sur l'ensemble de test\n",
        "\n",
        "Il est maintenant temps d'évaluer l'agorithme retenu, à savoir ici l'algorithme `Random Forest`, sur l'ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYQyWY7s9840"
      },
      "source": [
        "# Le meilleur paramétrage de l'algorithme Random Forest défini précédement \n",
        "model_final = grid_search.best_estimator_\n",
        "\n",
        "# Ensemble de test\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "\n",
        "# Les étiquettes \n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "# Application des transformations retenues plus en haut sur l'ensemble de test, en appliquant le Pipeline défini précédement\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "\n",
        "# Validation\n",
        "final_predictions = model_final.predict(X_test_prepared)\n",
        "\n",
        "# Calcul des erreurs de prédiction\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)   \n",
        "print(final_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLBzMugcTxXZ"
      },
      "source": [
        "# Exercice (Ne pas faire pour le moment)\n",
        "\n",
        "Appliquez la même démarche que précédement sur ce [dataset](https://github.com/securitylab-repository/TPS-IA/raw/master/payment_fraud.csv).\n",
        "\n",
        "Vous devez choisir entre les algorithmes:\n",
        "\n",
        "  - Régression logistique\n",
        "  - Les $K$ Plus proches voisins ($Knn$)"
      ]
    }
  ]
}