{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4-Introduction-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Ea62c-V7Aj"
      },
      "source": [
        "# The Perceptron algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfziS2rbV7Ap"
      },
      "source": [
        "In this section, we will look in detail at the Perceptron algorithm for learning a linear classifier in the case of binary labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrfDXn4HV7Ap"
      },
      "source": [
        "## 1. The algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zp5aNLIV7Ap"
      },
      "source": [
        "This first procedure, **evaluate_classifier**, takes as input the parameters of a linear classifier (`w,b`) as well as a data point (`x`) and returns the prediction of that classifier at `x`.\n",
        "\n",
        "The prediction is:\n",
        "* `1`  if `w.x+b > 0`\n",
        "* `0`  if `w.x+b = 0`\n",
        "* `-1` if `w.x+b < -1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7m7C3sbUcv7"
      },
      "source": [
        "> <font color=\"magenta\">Complete this function.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynNV_4nQUit2"
      },
      "source": [
        "def evaluate_classifier(w,b,x):\n",
        "  # add code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhXGPfaFV7Aq"
      },
      "source": [
        "Here is the Perceptron training procedure. It is invoked as follows:\n",
        "* `w,b,converged = train_perceptron(x,y,n_iters)`\n",
        "\n",
        "where\n",
        "* `x`: n-by-d numpy array with n data points, each d-dimensional\n",
        "* `y`: n-dimensional numpy array with the labels (each 1 or -1)\n",
        "* `n_iters`: the training procedure will run through the data at most this many times (default: 100)\n",
        "* `w,b`: parameters for the final linear classifier\n",
        "* `converged`: flag (True/False) indicating whether the algorithm converged within the prescribed number of iterations\n",
        "\n",
        "If the data is not linearly separable, then the training procedure will not converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owN9HEGsU8nR"
      },
      "source": [
        ">  <font color=\"magenta\">Regarding the perceptron update seen in the course, modify this function.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yGXhj8dRVIL7"
      },
      "source": [
        "def train_perceptron(x,y,n_iters=100):\n",
        "    n,d = x.shape\n",
        "    w = np.zeros((d,))\n",
        "    b = 0\n",
        "    done = False\n",
        "    converged = True\n",
        "    iters = 0\n",
        "    np.random.seed(None)\n",
        "    while not(done):\n",
        "        done = True\n",
        "        I = np.random.permutation(n)\n",
        "        for i in range(n):\n",
        "            j = I[i]\n",
        "            #if (?):\n",
        "             #   ?\n",
        "             #   ?\n",
        "             #   done = False\n",
        "        iters = iters + 1\n",
        "        if iters > n_iters:\n",
        "            done = True\n",
        "            converged = False\n",
        "    if converged:\n",
        "        print \"Perceptron algorithm: iterations until convergence: \", iters\n",
        "    else:\n",
        "        print \"Perceptron algorithm: did not converge within the specified number of iterations\"\n",
        "    return w, b, converged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp4OMBOvV7Ar"
      },
      "source": [
        "## 2. Experiments with the Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmG5PYMGV7Ar"
      },
      "source": [
        "We start with standard includes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WCjAZw4zV7As"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rc('xtick', labelsize=14) \n",
        "matplotlib.rc('ytick', labelsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bID1AAx9V7As"
      },
      "source": [
        "The directory containing this notebook should also contain the two-dimensional data files, [data_1.txt](https://raw.githubusercontent.com/securitylab-repository/TPS-IA/master/data_1.txt) and [data_2.txt](https://raw.githubusercontent.com/securitylab-repository/TPS-IA/master/data_2.txt). These files contain one data point per line, along with a label, like:\n",
        "* `3 8 1` (meaning that point `x=(3,8)` has label `y=1`)\n",
        "\n",
        "The next procedure, **run_perceptron**, loads one of these data sets, learns a linear classifier using the Perceptron algorithm, and then displays the data as well as the boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cvrqCBRcV7As"
      },
      "source": [
        "def run_perceptron(datafile):\n",
        "    data = np.loadtxt(datafile)\n",
        "    n,d = data.shape\n",
        "    # Create training set x and labels y\n",
        "    x = data[:,0:2]\n",
        "    y = data[:,2]\n",
        "    # Run the Perceptron algorithm for at most 100 iterations\n",
        "    w,b,converged = train_perceptron(x,y,100)\n",
        "    # Determine the x1- and x2- limits of the plot\n",
        "    x1min = min(x[:,0]) - 1\n",
        "    x1max = max(x[:,0]) + 1\n",
        "    x2min = min(x[:,1]) - 1\n",
        "    x2max = max(x[:,1]) + 1\n",
        "    plt.xlim(x1min,x1max)\n",
        "    plt.ylim(x2min,x2max)\n",
        "    # Plot the data points\n",
        "    plt.plot(x[(y==1),0], x[(y==1),1], 'ro')\n",
        "    plt.plot(x[(y==-1),0], x[(y==-1),1], 'k^')\n",
        "    # Construct a grid of points at which to evaluate the classifier\n",
        "    if converged:\n",
        "        grid_spacing = 0.05\n",
        "        xx1, xx2 = np.meshgrid(np.arange(x1min, x1max, grid_spacing), np.arange(x2min, x2max, grid_spacing))\n",
        "        grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "        Z = np.array([evaluate_classifier(w,b,pt) for pt in grid])\n",
        "        # Show the classifier's boundary using a color plot\n",
        "        Z = Z.reshape(xx1.shape)\n",
        "        plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.PRGn, vmin=-3, vmax=3)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTpQRlP_V7At"
      },
      "source": [
        "Let's run this on `data_1.txt`. Try running it a few times; you should get slightly different outcomes. \n",
        "\n",
        ">  <font color=\"magenta\">Why ?</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijgiGXY1V7At"
      },
      "source": [
        "run_perceptron('data_1.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8XwZHX-V7At"
      },
      "source": [
        "And now, let's try running it on `data_2.txt`. \n",
        ">  <font color=\"magenta\">What's going on here?</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOucXbV5V7At"
      },
      "source": [
        "run_perceptron('data_2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDMeQYZXTeH"
      },
      "source": [
        "# Sentiment analysis with support vector machines\n",
        "\n",
        "In this notebook, we will work on the learning task that: predicting the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. \n",
        "\n",
        "Before starting on this notebook, make sure the file [full_set.txt](https://raw.githubusercontent.com/securitylab-repository/TPS-IA/master/full_set.txt) is in the same directory. Recall that the data can be downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l05AiYqXTeM"
      },
      "source": [
        "## 1. Loading and preprocessing the data\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "--q-M9mKXTeM"
      },
      "source": [
        "%matplotlib inline\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rc('xtick', labelsize=14) \n",
        "matplotlib.rc('ytick', labelsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxCnR6KoCZ-S"
      },
      "source": [
        "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from course, we will change the negative review label to '-1'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1eBP9s9XTeN"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Read in the data set.\n",
        "with open(\"full_set.txt\") as f:\n",
        "    content = f.readlines()\n",
        "    \n",
        "## Remove leading and trailing white space\n",
        "content = [x.strip() for x in content]\n",
        "\n",
        "## Separate the sentences from the labels\n",
        "sentences = [x.split(\"\\t\")[0] for x in content]\n",
        "labels = [x.split(\"\\t\")[1] for x in content]\n",
        "\n",
        "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
        "y = np.array(labels, dtype='int8')\n",
        "y = 2*y - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0wg5wM_DxO4"
      },
      "source": [
        "### Preprocessing the text data\n",
        "\n",
        "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
        "\n",
        "1. Remove punctuation and numbers.\n",
        "2. Transform all words to lower-case.\n",
        "3. Remove _stop words_.\n",
        "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
        "\n",
        "We begin with first two steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC_7V1T4Cln9"
      },
      "source": [
        "## full_remove takes a string x and a list of characters removal_list \n",
        "## returns x with all the characters in removal_list replaced by ' '\n",
        "def full_remove(x, removal_list):\n",
        "    for w in removal_list:\n",
        "        x = x.replace(w, ' ')\n",
        "    return x\n",
        "\n",
        "## Remove digits\n",
        "digits = [str(x) for x in range(10)]\n",
        "digit_less = [full_remove(x, digits) for x in sentences]\n",
        "\n",
        "## Remove punctuation\n",
        "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
        "\n",
        "## Make everything lower-case\n",
        "sents_lower = [x.lower() for x in punc_less]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53shzzkEHa8"
      },
      "source": [
        "### Stop words\n",
        "\n",
        "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2K3GJgOCx5w"
      },
      "source": [
        "## Define our stop words\n",
        "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
        "\n",
        "## Remove stop words\n",
        "sents_split = [x.split() for x in sents_lower]\n",
        "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahdwHJy4EUnW"
      },
      "source": [
        ">  <font color=\"magenta\">What do the sentences look like so far?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPNC05ilEp-8"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. \n",
        "\n",
        "In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., V}` where `V` is the size of our vocabulary. And each sentence is represented as a V-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
        "\n",
        "To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn`. We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
        "\n",
        "Finally, we will also append a '1' to the end of each vector to allow our linear classifier to learn a bias term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Plr4bpdEEmV"
      },
      "source": [
        "## Transform to bag of words representation.\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
        "data_features = vectorizer.fit_transform(sents_processed)\n",
        "\n",
        "## Append '1' to the end of each vector.\n",
        "data_mat = data_features.toarray()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUQSGN8vEz-c"
      },
      "source": [
        "### Training / test split\n",
        "\n",
        ">  <font color=\"magenta\">Split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH3_B0aXXTeN"
      },
      "source": [
        "## 2. Fitting a support vector machine (SVM) to the data\n",
        "\n",
        "In support vector machines, we are given a set of examples $(x_1, y_1), \\ldots, (x_n, y_n)$ and we want to find a weight vector $w \\in \\mathbb{R}^d$ that solves the following optimization problem:\n",
        "\n",
        "$$ \\min_{w \\in \\mathbb{R}^d} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i $$\n",
        "$$ \\text{subject to } y_i \\langle w, x_i \\rangle \\geq 1 - \\xi_i \\text{ for all } i=1,\\ldots, n$$\n",
        "\n",
        "`scikit-learn` provides an SVM solver that we will use. The following routine takes as input the constant `C` (from the above optimization problem) and returns the training and test error of the resulting SVM model. It is invoked as follows:\n",
        "\n",
        "* `training_error, test_error = fit_classifier(C)`\n",
        "\n",
        "The default value for parameter `C` is 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCkl814wmjh5"
      },
      "source": [
        ">  <font color=\"magenta\"> Complete this code. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Q-34ES8ShwKF"
      },
      "source": [
        "from sklearn import svm\n",
        "def fit_classifier(C_value=1.0):\n",
        "    clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
        "    # training step\n",
        "    # add code here ?\n",
        "    \n",
        "    # Get predictions on training data\n",
        "    # train_preds = ?\n",
        "    train_error = float(np.sum((train_preds > 0.0) != (train_labels > 0.0)))/len(train_labels)\n",
        "    ## Get predictions on test data\n",
        "    #test_preds = ? \n",
        "    test_error = float(np.sum((test_preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
        "    ##\n",
        "    return train_error, test_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlkkGz7zm97h"
      },
      "source": [
        "> <font color = 'magenta'> Train a model with these values cvals = [0.01,0.1,1.0,10.0,100.0,1000.0,10000.0] and print the train and test error corresponding to each value </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rNXYeFkjgrC"
      },
      "source": [
        "> <font color = 'magenta'>Are the results consistent ? Explain.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NptUsA4XTeO"
      },
      "source": [
        "### Evaluating C by k-fold cross-validation\n",
        "\n",
        "As we can see, the choice of `C` has a very significant effect on the performance of the SVM classifier. We were able to assess this because we have a separate test set. In general, however, this is a luxury we won't possess. How can we choose `C` based only on the training set\n",
        "\n",
        "A reasonable way to estimate the error associated with a specific value of `C` is by **`k-fold cross validation`**\n",
        "\n",
        "> <font color = 'magenta'>Explain this method ?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEGeZuKQlmPG"
      },
      "source": [
        "The following procedure, **cross_validation_error**, does exactly this. It takes as input:\n",
        "* the training set `x,y`\n",
        "* the value of `C` to be evaluated\n",
        "* the integer `k`\n",
        "\n",
        "and it returns the estimated error of the classifier for that particular setting of `C`. <font color=\"magenta\">Look over the code carefully to understand exactly what it is doing.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Nm8x6MwsXTeO"
      },
      "source": [
        "def cross_validation_error(x,y,C_value,k):\n",
        "    n = len(y)\n",
        "    ## Randomly shuffle indices\n",
        "    indices = np.random.permutation(n)\n",
        "    \n",
        "    ## Initialize error\n",
        "    err = 0.0\n",
        "    \n",
        "    ## Iterate over partitions\n",
        "    for i in range(k):\n",
        "        ## Partition indices\n",
        "        test_indices = indices[int(i*(n/k)):int((i+1)*(n/k) - 1)]\n",
        "        train_indices = np.setdiff1d(indices, test_indices)\n",
        "        \n",
        "        ## Train classifier with parameter c\n",
        "        clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
        "        clf.fit(x[train_indices], y[train_indices])\n",
        "        \n",
        "        ## Get predictions on test partition\n",
        "        preds = clf.predict(x[test_indices])\n",
        "        \n",
        "        ## Compute error\n",
        "        err += float(np.sum((preds > 0.0) != (y[test_indices] > 0.0)))/len(test_indices)\n",
        "        \n",
        "    return err/k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "319BcTJcXTeP"
      },
      "source": [
        "The procedure **cross_validation_error** (above) evaluates a single candidate value of `C`. We need to use it repeatedly to identify a good `C`. \n",
        "\n",
        "> Write a function to choose `C`. It will be invoked as follows:\n",
        "* `c, err = choose_parameter(x,y,k)`\n",
        "where\n",
        "* `x,y` is the training data\n",
        "* `k` is the number of folds of cross-validation\n",
        "* `c` is chosen value of the parameter `C`\n",
        "* `err` is the cross-validation error estimate at `c`\n",
        "\n",
        "<font color=\"magenta\">Note:</font> This is a tricky business because a priori, even the order of magnitude of `C` is unknown. Should it be 0.0001 or 10000? You might want to think about trying multiple values that are arranged in a geometric progression (such as powers of ten). *In addition to returning a specific value of `C`, your function should **plot** the cross-validation errors for all the values of `C` it tried out (possibly using a log-scale for the `C`-axis).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CUNWNMw6XTeP"
      },
      "source": [
        "def choose_parameter(x,y,k):\n",
        "    ### Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQuSmjusXTeQ"
      },
      "source": [
        "Now let's try out your routine!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pusEbrkXTeQ"
      },
      "source": [
        "c, err = choose_parameter(train_data, train_labels, 10)\n",
        "print(\"Choice of C: \", c)\n",
        "print(\"Cross-validation error estimate: \", err)\n",
        "## Train it and test it\n",
        "clf = svm.LinearSVC(C=c, loss='hinge')\n",
        "clf.fit(train_data, train_labels)\n",
        "preds = clf.predict(test_data)\n",
        "error = float(np.sum((preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
        "print(\"Test error: \", error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MspMhzknXTeQ"
      },
      "source": [
        "> How does the plot of cross-validation errors for different `C` look? Is there clearly a trough in which the returned value of `C` falls? Does the plot provide some reassurance that the choice is reasonable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MHgB_cCFF0R"
      },
      "source": [
        "## 3. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbdOALXFFyi0"
      },
      "source": [
        "## 2. Fitting a logistic regression model to the training data\n",
        "\n",
        "We could implement our own logistic regression solver using stochastic gradient descent, but fortunately, there is already one built into `scikit-learn`.\n",
        "\n",
        "Due to the randomness in the SGD procedure, different runs can yield slightly different solutions (and thus different error values)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc3j2qvlGlH8"
      },
      "source": [
        "> <font color = 'magenta'> Use  `SGDClassifier` from `sklearn.linear_model`  class to train a logistic regression model. Complete the following code:</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Nu-xK4xEXTeQ"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "## Fit logistic classifier on training data\n",
        "#clf = ? \n",
        "# ? \n",
        "\n",
        "## Pull out the parameters (w,b) of the logistic regression model\n",
        "w = clf.coef_[0,:]\n",
        "b = clf.intercept_\n",
        "\n",
        "## Get predictions on training and test data\n",
        "#preds_train = ?\n",
        "#preds_test = ?\n",
        "\n",
        "## Compute errors\n",
        "#errs_train = ?\n",
        "#errs_test = ?\n",
        "\n",
        "print \"Training error: \", float(errs_train)/len(train_labels)\n",
        "print \"Test error: \", float(errs_test)/len(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjM0el9Uo7UY"
      },
      "source": [
        "\n",
        "The logistic regression model produces not just classifications but also conditional probability estimates. \n",
        "\n",
        "> <font color = 'magenta'> Compute probability on each test point</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TxrAMXcfhhg"
      },
      "source": [
        "4. \n",
        "\n",
        "> <font color = 'magenta'>Your turn. Do the same things with:</font>\n",
        "\n",
        "  - <font color = 'magenta'>Decision Tree</font>\n",
        "  -<font color = 'magenta'> Knn  </font>"
      ]
    }
  ]
}